Process rank: -1, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False
Training/evaluation parameters Namespace(adam_epsilon=1e-08, adv_epsilon=1.0, adv_name='word_embeddings', cache_dir='', config_name='', crf_learning_rate=5e-05, data_dir='/home/sangeng/Downloads/myproject/baseline/BERT-NER-Pytorch-master/datasets/cner/', device=device(type='cpu'), do_adv=False, do_eval=True, do_lower_case=True, do_predict=False, do_train=True, eval_all_checkpoints=False, eval_max_seq_length=512, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, id2label={0: 'X', 1: 'B-CONT', 2: 'B-EDU', 3: 'B-LOC', 4: 'B-NAME', 5: 'B-ORG', 6: 'B-PRO', 7: 'B-RACE', 8: 'B-TITLE', 9: 'I-CONT', 10: 'I-EDU', 11: 'I-LOC', 12: 'I-NAME', 13: 'I-ORG', 14: 'I-PRO', 15: 'I-RACE', 16: 'I-TITLE', 17: 'O', 18: 'S-NAME', 19: 'S-ORG', 20: 'S-RACE', 21: '[START]', 22: '[END]'}, label2id={'X': 0, 'B-CONT': 1, 'B-EDU': 2, 'B-LOC': 3, 'B-NAME': 4, 'B-ORG': 5, 'B-PRO': 6, 'B-RACE': 7, 'B-TITLE': 8, 'I-CONT': 9, 'I-EDU': 10, 'I-LOC': 11, 'I-NAME': 12, 'I-ORG': 13, 'I-PRO': 14, 'I-RACE': 15, 'I-TITLE': 16, 'O': 17, 'S-NAME': 18, 'S-ORG': 19, 'S-RACE': 20, '[START]': 21, '[END]': 22}, learning_rate=3e-05, local_rank=-1, logging_steps=-1, loss_type='ce', markup='bios', max_grad_norm=1.0, max_steps=-1, model_name_or_path='/home/sangeng/Downloads/myproject/baseline/BERT-NER-Pytorch-master/prev_trained_model/bert-base-chinese', model_type='bert', n_gpu=0, no_cuda=False, num_train_epochs=3.0, output_dir='/home/sangeng/Downloads/myproject/baseline/BERT-NER-Pytorch-master/outputs/cner_output/bert', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=24, per_gpu_train_batch_size=24, predict_checkpoints=0, save_steps=-1, seed=42, server_ip='', server_port='', task_name='cner', tokenizer_name='', train_max_seq_length=128, warmup_proportion=0.1, weight_decay=0.01)
Loading features from cached file /home/sangeng/Downloads/myproject/baseline/BERT-NER-Pytorch-master/datasets/cner/cached_soft-train_bert-base-chinese_128_cner
***** Running training *****
  Num examples = 3820
  Num Epochs = 3
  Instantaneous batch size per GPU = 24
  Total train batch size (w. parallel, distributed & accumulation) = 24
  Gradient Accumulation steps = 1
  Total optimization steps = 480
